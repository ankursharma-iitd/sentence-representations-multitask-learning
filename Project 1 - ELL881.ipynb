{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from collections import OrderedDict\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required for NLI Task\n",
    "untokenized_premises = './data/part1/premises.txt'\n",
    "untokenized_hypothesis = './data/part1/hypothesis.txt'\n",
    "tokenized_premises = './data/part1/tokenized_premises.txt'\n",
    "tokenized_hypothesis = './data/part1/tokenized_hypothesis.txt'\n",
    "untokenized_premises_dev = './data/part1/premises_dev.txt'\n",
    "untokenized_hypothesis_dev = './data/part1/hypothesis_dev.txt'\n",
    "tokenized_premises_dev = './data/part1/tokenized_premises_dev.txt'\n",
    "tokenized_hypothesis_dev = './data/part1/tokenized_hypothesis_dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required for CP task\n",
    "untokenized_source_file_CP = './data/part2/en.txt'\n",
    "untokenized_target_file_CP = './data/part2/pt.txt'\n",
    "tokenized_source_file_CP = './data/part2/source.txt'\n",
    "tokenized_target_file_CP = './data/part2/target.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required for NMT task\n",
    "untokenized_source_file_NMT = './data/part3/english.txt'\n",
    "untokenized_target_file_NMT = './data/part3/german.txt'\n",
    "tokenized_source_file_NMT = './data/part3/source.txt'\n",
    "tokenized_target_file_NMT = './data/part3/target.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the input files\n",
    "def convert_to_tokens(input_file, output_file):\n",
    "    with open(input_file) as fr,open(output_file, 'w') as fw:\n",
    "        for index, sentence in enumerate(fr):\n",
    "            sentence = ' '.join(tokenizer.tokenize(sentence))\n",
    "            words = word_tokenize(sentence.strip().lower())\n",
    "            fw.write(f\"{' '.join(words)}\\n\")\n",
    "            if index % 100000 == 0:\n",
    "                print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the punctuations - used in the CP task\n",
    "import string\n",
    "def process_string(s):\n",
    "    exclude = set(string.punctuation)\n",
    "    out = ''\n",
    "    for ch in s:\n",
    "        if(ch in exclude):\n",
    "            out += ' ' + ch + ' '\n",
    "        else:\n",
    "            out += ch\n",
    "    return ' '.join(out.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens_CP(input_file, output_file):\n",
    "    with open(input_file) as fr,open(output_file, 'w') as fw:\n",
    "        for index, sentence in enumerate(fr):\n",
    "            sentence = process_string(sentence)\n",
    "            fw.write(f\"{sentence}\\n\")\n",
    "            if index % 100000 == 0:\n",
    "                print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLI Task\n",
    "convert_to_tokens(untokenized_premises, tokenized_premises)\n",
    "convert_to_tokens(untokenized_premises_dev, tokenized_premises_dev)\n",
    "convert_to_tokens(untokenized_hypothesis, tokenized_hypothesis)\n",
    "convert_to_tokens(untokenized_hypothesis_dev, tokenized_hypothesis_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CP Task\n",
    "convert_to_tokens_CP(untokenized_source_file_CP, tokenized_source_file_CP)\n",
    "convert_to_tokens_CP(untokenized_target_file_CP, tokenized_target_file_CP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMT\n",
    "convert_to_tokens(untokenized_source_file_NMT, tokenized_source_file_NMT)\n",
    "convert_to_tokens(untokenized_target_file_NMT, tokenized_target_file_NMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count words and return words in a sorted order\n",
    "def count_words(counter, sentences_file):\n",
    "    for sentence in open(sentences_file):\n",
    "        words = sentence[2:].strip().split()\n",
    "        for word in words:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Vocab\n",
    "def build_vocab(word_counts):\n",
    "    vocab = OrderedDict()\n",
    "    vocab['PAD'] = 0\n",
    "    vocab['EOS'] = 1\n",
    "    vocab['UNK'] = 2\n",
    "    vocab['GO'] = 3\n",
    "    count = 4\n",
    "    for word, freq in word_counts:\n",
    "        if(count == 30000):\n",
    "            break\n",
    "        vocab[word] = len(vocab) + 1\n",
    "        count += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLI \n",
    "counter = dict()\n",
    "counter = count_words(counter, tokenized_premises)\n",
    "counter = count_words(counter, tokenized_hypothesis)\n",
    "counter = count_words(counter, tokenized_source_file_CP)\n",
    "counter = count_words(counter, tokenized_target_file_CP)\n",
    "english_counter = count_words(counter, tokenized_source_file_NMT)\n",
    "english_word_counts = sorted(english_counter.items(), key=lambda pair:pair[1], reverse=True)\n",
    "english_vocab = build_vocab(english_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_counter = dict()\n",
    "german_counter = count_words(german_counter, tokenized_target_file_NMT)\n",
    "german_word_counts = sorted(german_counter.items(), key=lambda pair:pair[1], reverse=True)\n",
    "german_vocab = build_vocab(german_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_file = './data/english_vocab.txt'\n",
    "german_vocab_file = './data/german_vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(vocab_file, vocab):\n",
    "    with open(vocab_file, 'w') as fw:\n",
    "        for word in vocab:\n",
    "            fw.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(english_vocab_file, english_vocab)\n",
    "write_vocab_file(german_vocab_file, german_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedder used in this project\n",
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, V, d):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "    \n",
    "    def call(self, word_indexes):\n",
    "        return tf.cast(tf.nn.embedding_lookup(self.W, word_indexes), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the english and german vocabs from the file\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "english_vocab_table = lookup_ops.index_table_from_file(english_vocab_file, default_value=0)\n",
    "german_vocab_table = lookup_ops.index_table_from_file(german_vocab_file, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the file names\n",
    "sentences_file_NLI = './data/part1/training.txt'\n",
    "shuffled_sentences_file_NLI = './data/part1/shuffled_training.txt'\n",
    "check_sentences_file_NLI = './data/part1/check_training.txt'\n",
    "valid_file_NLI = './data/part1/valid.txt'\n",
    "sentences_file_CP = './data/part2/training.txt'\n",
    "valid_file_CP = './data/part2/valid.txt'\n",
    "shuffled_sentences_file_CP = './data/part2/shuffled_training.txt'\n",
    "check_sentences_file_CP = './data/part2/check_training.txt'\n",
    "sentences_file_NMT = './data/part3/training.txt'\n",
    "valid_file_NMT = './data/part3/valid.txt'\n",
    "shuffled_sentences_file_NMT = './data/part3/shuffled_training.txt'\n",
    "check_sentences_file_NMT = './data/part3/check_training.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine source and target files for the NLI task\n",
    "def combine_files_NLI(sentences_file, premises, hypothesis):\n",
    "    with open(premises) as fp, open(hypothesis) as fh, open(sentences_file, 'w') as fw:\n",
    "        fp = list(fp)\n",
    "        fh = list(fh)\n",
    "        for i in range(len(list(fp))):\n",
    "            premise = ((fp[i])[2:]).strip()\n",
    "            hypothesis = ((fh[i])[2:]).strip()\n",
    "            label = ((fp[i])[0]).strip()\n",
    "            fw.write(f\"{','.join([label, premise, hypothesis])}\\n\")\n",
    "combine_files_NLI(sentences_file_NLI, tokenized_premises, tokenized_hypothesis)\n",
    "combine_files_NLI(valid_file_NLI, tokenized_premises_dev, tokenized_hypothesis_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the source and target files for NMT task\n",
    "def combine_files_CP_NMT(sentences_file, source_file, target_file):\n",
    "    with open(source_file) as fp, open(target_file) as fh, open(sentences_file, 'w') as fw:\n",
    "        fp = list(fp)\n",
    "        fh = list(fh)\n",
    "        for i in range(len(list(fp))):\n",
    "            sent = fp[i].strip()\n",
    "            tree = fh[i].strip()\n",
    "            if((sent == '') or (tree == '')):\n",
    "                continue\n",
    "            concat = '\\t'.join([sent, tree])\n",
    "            fw.write(f\"{concat}\\n\")\n",
    "combine_files_CP_NMT(sentences_file_CP, tokenized_source_file_CP, tokenized_target_file_CP)\n",
    "combine_files_CP_NMT(sentences_file_NMT, tokenized_source_file_NMT, tokenized_target_file_NMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the check, and valid dataset - small datasets for demo purposes/fast training\n",
    "training_samples = 256\n",
    "valid_samples = 128\n",
    "import os\n",
    "def shuffle_sentence_file(file_name, shuffled_file, check_file, valid_file):\n",
    "    os.system(\"gshuf \" + file_name + \" > \" + shuffled_file)\n",
    "    os.system(\"head -\" + str(training_samples) + \" \" + file_name + \" > \" + check_file)\n",
    "    os.system(\"tail -\" + str(valid_samples) + \" \" + file_name + \" > \" + valid_file)\n",
    "shuffle_sentence_file(sentences_file_NLI, shuffled_sentences_file_NLI, check_sentences_file_NLI, valid_file_NLI)\n",
    "shuffle_sentence_file(sentences_file_CP, shuffled_sentences_file_CP, check_sentences_file_CP, valid_file_CP)\n",
    "shuffle_sentence_file(sentences_file_NMT, shuffled_sentences_file_NMT, check_sentences_file_NMT, valid_file_NMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the datasets\n",
    "\n",
    "#(label, sent1, len1, sent2, len2)\n",
    "def create_dataset_NLI(sentences_file, vocab_table, batch_size):\n",
    "    dataset = tf.data.TextLineDataset(sentences_file)\n",
    "    dataset = dataset.map(lambda sentence: (\n",
    "        tf.cast(tf.string_to_number((tf.string_split([sentence],',')).values[0]), tf.int64), \n",
    "        vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[1]]).values),\n",
    "        tf.size(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[1]]).values)),\n",
    "        vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[2]]).values),\n",
    "        tf.size(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[2]]).values))\n",
    "                         ))\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([], [None], [], [None], []))\n",
    "    return dataset\n",
    "\n",
    "#(sent1, len1, sent2, sent2_shifted, len2)\n",
    "def create_dataset_CP(sentences_file, vocab_table, batch_size):\n",
    "    dataset = tf.data.TextLineDataset(sentences_file)\n",
    "    dataset = dataset.filter(lambda sentence : (tf.size(english_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values)) <= 100))\n",
    "    dataset = dataset.map(lambda sentence: (\n",
    "        tf.cast(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[0]]).values), dtype=tf.int32),\n",
    "        tf.size(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[0]]).values)),\n",
    "        tf.cast(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values), dtype=tf.int32),\n",
    "        tf.cast(vocab_table.lookup(tf.concat(((tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values)[1:],['EOS']), axis=0) ), dtype=tf.int32),\n",
    "        tf.size(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values))\n",
    "                         ))\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([None], [], [None], [None], []))\n",
    "    return dataset\n",
    "\n",
    "#(sent1, len1, sent2, sent2_shifted, len2)\n",
    "def create_dataset_NMT(sentences_file, english_vocab_table, german_vocab_table, batch_size):\n",
    "    dataset = tf.data.TextLineDataset(sentences_file)\n",
    "    dataset = dataset.filter(lambda sentence : (tf.size(english_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values)) <= 100))\n",
    "    dataset = dataset.map(lambda sentence: (\n",
    "        english_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[0]]).values),\n",
    "        tf.size(english_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[0]]).values)),\n",
    "        german_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values),\n",
    "        german_vocab_table.lookup(tf.concat(((tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values)[1:],['EOS']), axis=0) ),\n",
    "        tf.size(german_vocab_table.lookup(tf.string_split([(tf.string_split([sentence],'\\t')).values[1]]).values))\n",
    "                         ))\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([None], [], [None], [None], []))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for the nli part\n",
    "dataset_NLI = create_dataset_NLI(sentences_file_NLI, english_vocab_table, batch_size)\n",
    "valid_dataset_NLI = create_dataset_NLI(valid_file_NLI, english_vocab_table, batch_size)\n",
    "dataset_NLI = dataset_NLI.shuffle(buffer_size=10000)\n",
    "valid_dataset_NLI = valid_dataset_NLI.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static rnn\n",
    "class StaticRNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(StaticRNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, state, word_vectors, num_words):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs, final_state = tf.nn.static_rnn(cell=self.cell, initial_state = state, inputs=word_vectors_time, sequence_length=num_words, dtype=tf.float32)\n",
    "        return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same encoder has been used for all the three parts\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embedding = Embedding(V, d)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        \n",
    "    def call(self, word_vector, word_length):\n",
    "        word_vectors = self.word_embedding(word_vector)\n",
    "        rnn_outputs_time, final_state = self.rnn(None, word_vectors, word_length)\n",
    "        output = []\n",
    "        #take only the outputs at the end of the step of that particular sentence\n",
    "        for i in range(int(tf.size(word_length))):\n",
    "            output.append(rnn_outputs_time[int(word_length[i]) - 1][i])\n",
    "        t = tf.convert_to_tensor(output, dtype=tf.float32)\n",
    "        return t, final_state, self.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for NLI - Task (A multilayer perceptron)\n",
    "n_hidden_1 = 512 # 1st layer number of features\n",
    "n_input = 2048 \n",
    "n_classes = 3 \n",
    "keep_prob = 0.7 #drop out of 0.3 in the hidden layer\n",
    "\n",
    "# Store layers weight &amp; bias - xavier initialisation has been used\n",
    "weights = {\n",
    "'h1': tf.get_variable(\"W\", shape=[n_input, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "'out': tf.get_variable(\"W\", shape=[n_hidden_1, n_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'b1': tf.get_variable(\"b1\", shape=[n_hidden_1], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "'out': tf.get_variable(\"out\", shape=[n_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "}  \n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    drop_out = tf.nn.dropout(layer_1, keep_prob)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(drop_out, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder for CP - Task, old embeddings has been used for this word vector\n",
    "class Decoder_CP(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, h, cell):\n",
    "        super(Decoder_CP, self).__init__()\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, embeddings, encoder_final_state, word_vector, word_length):\n",
    "        decoder_inputs_embedded = embeddings(word_vector)\n",
    "        decoder_outputs,_ = self.rnn(encoder_final_state, decoder_inputs_embedded, word_length)\n",
    "        rnn_outputs = tf.stack(decoder_outputs, axis=1)\n",
    "        logits = self.output_layer(rnn_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder model for the NMT task - notice that a new word embedding has been created for german language\n",
    "class Decoder_NMT(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d, h, cell):\n",
    "        super(Decoder_NMT, self).__init__()\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        self.word_embedding = Embedding(vocab_size, d)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, english_embeddings, encoder_final_state, word_vector, word_length):\n",
    "        german_embeddings = self.word_embedding(word_vector)\n",
    "        decoder_outputs,_ = self.rnn(encoder_final_state, german_embeddings, word_length)\n",
    "        rnn_outputs = tf.stack(decoder_outputs, axis=1)\n",
    "        logits = self.output_layer(rnn_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function for each of the parts\n",
    "def loss_fun_nli(encoder, datum):\n",
    "    premise_logits,_,_ = encoder(datum[1], datum[2])\n",
    "    hypothesis_logits,_,_ = encoder(datum[3], datum[4])\n",
    "    concatenated_logits = tf.concat([tf.concat([premise_logits,hypothesis_logits], axis = 1), \n",
    "                                tf.abs(tf.subtract(premise_logits, hypothesis_logits)), \n",
    "                                tf.multiply(premise_logits, hypothesis_logits)], axis = 1\n",
    "                               )\n",
    "    logits = multilayer_perceptron(concatenated_logits, weights, biases)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[0])\n",
    "    return (tf.reduce_sum(loss) / int(tf.reduce_sum(datum[2])))\n",
    "\n",
    "def loss_fun_cp(encoder, decoder, datum):\n",
    "    _, encoder_final_state, encoder_embeddings = encoder(datum[0], datum[1])\n",
    "    logits = decoder(encoder_embeddings, encoder_final_state, datum[2], datum[4])\n",
    "    mask = tf.sequence_mask(datum[4], dtype=tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[3]) * mask\n",
    "    return (tf.reduce_sum(loss) / int(tf.reduce_sum(datum[4])))\n",
    "\n",
    "def loss_fun_nmt(encoder, decoder, datum):\n",
    "    _, encoder_final_state,_ = encoder(datum[0], datum[1])\n",
    "    logits = decoder(encoder_final_state, datum[2], datum[4])\n",
    "    mask = tf.sequence_mask(datum[4], dtype=tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[3]) * mask\n",
    "    return (tf.reduce_sum(loss) / int(tf.reduce_sum(datum[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for clipping gradients\n",
    "def clip_gradients(grads_and_vars, clip_ratio):\n",
    "  gradients, variables = zip(*grads_and_vars)\n",
    "  clipped, _ = tf.clip_by_global_norm(gradients, clip_ratio)\n",
    "  return zip(clipped, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for printing the log with the timestamp\n",
    "logging = tf.logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "def log_msg(msg):\n",
    "       logging.info(f'{time.ctime()}: {msg}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the perplexity given the models, and the dataset\n",
    "def compute_ppl_nli(model, dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(dataset):\n",
    "        num_words = int(tf.reduce_sum(datum[2]))\n",
    "        avg_loss = loss_fun_nli(model, datum)\n",
    "        total_loss = avg_loss * num_words\n",
    "        total_words += num_words\n",
    "        if batch_num % 50 == 0:\n",
    "            log_msg(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / total_words\n",
    "    return np.exp(loss)\n",
    "\n",
    "def compute_ppl_cp(encoder, decoder, valid_dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(valid_dataset):\n",
    "        num_words = int(tf.reduce_sum(datum[4]))\n",
    "        avg_loss = loss_fun_cp(encoder, decoder, datum)\n",
    "        total_loss = avg_loss * num_words\n",
    "        total_words += num_words\n",
    "        if batch_num % 50 == 0:\n",
    "            log_msg(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / total_words\n",
    "    return np.exp(loss)\n",
    "\n",
    "def compute_ppl_nmt(encoder, decoder, valid_dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(valid_dataset):\n",
    "        num_words = int(tf.reduce_sum(datum[4]))\n",
    "        avg_loss = loss_fun_nmt(encoder, decoder, datum)\n",
    "        total_loss = avg_loss * num_words\n",
    "        total_words += num_words\n",
    "        if batch_num % 50 == 0:\n",
    "            log_msg(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / total_words\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the encoder for NLI part, deocder is actually the MLP\n",
    "import numpy as np\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "encoder_nli = Encoder(english_vocab_table.size(), 256, 512, 'gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss value & graditent function using the original loss function\n",
    "loss_and_grads_fun_nli = tfe.implicit_value_and_gradients(loss_fun_nli)\n",
    "loss_and_grads_fun_cp = tfe.implicit_value_and_gradients(loss_fun_cp)\n",
    "loss_and_grads_fun_nmt = tfe.implicit_value_and_gradients(loss_fun_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the model at this checkpoint whenever the perplexity improves\n",
    "import os\n",
    "checkpoint_dir = './encoder_nli'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_nli, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the NLI Model\n",
    "NUM_EPOCHS = 1\n",
    "STATS_STEPS = 50\n",
    "EVAL_STEPS = 500\n",
    "\n",
    "valid_ppl = compute_ppl_nli(encoder_nli, valid_dataset_NLI)\n",
    "print(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    dataset_NLI = dataset_NLI.shuffle(buffer_size=10000)\n",
    "    for step_num, datum in enumerate(dataset_NLI, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun_nli(encoder_nli, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl_nli(encoder_nli, valid_dataset_NLI)\n",
    "            \n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "                \n",
    "        \n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training and validation dataset for the CP part, and shuffle them\n",
    "dataset_CP = create_dataset_CP(sentences_file_CP, english_vocab_table, 32)\n",
    "valid_dataset_CP = create_dataset_CP(valid_file_CP, english_vocab_table, 32)\n",
    "dataset_CP = dataset_CP.shuffle(buffer_size=10000)\n",
    "valid_dataset_CP = valid_dataset_CP.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models for the Task - 2 - CP\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "encoder_cp = Encoder(english_vocab_table.size(), 256, 512, 'gru')\n",
    "decoder_cp = Decoder_CP(english_vocab_table.size(), 512, 'gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back the trained model after the nli part\n",
    "checkpoint_dir = './encoder_nli'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_cp, optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the new checkpoint to store the trained model after the cp part\n",
    "import os\n",
    "checkpoint_dir = './encoder_cp'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_cp, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model for the CP part\n",
    "NUM_EPOCHS = 1\n",
    "STATS_STEPS = 10\n",
    "EVAL_STEPS = 100\n",
    "\n",
    "valid_ppl = compute_ppl_cp(encoder_cp, decoder_cp, valid_dataset_CP)\n",
    "log_msg(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    dataset_CP = dataset_CP.shuffle(buffer_size = 10000)\n",
    "    for step_num, datum in enumerate(dataset_CP, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun_cp(encoder_cp, decoder_cp, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            log_msg(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl_cp(encoder_cp, decoder_cp, valid_dataset_CP)\n",
    "            \n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "                log_msg(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                log_msg(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "                \n",
    "        \n",
    "    log_msg(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the NMT training set\n",
    "dataset_NMT = create_dataset_NMT(sentences_file_NMT, english_vocab_table, german_vocab_table, 32)\n",
    "valid_dataset_NMT = create_dataset_NMT(valid_file_NMT, english_vocab_table, german_vocab_table, 32)\n",
    "dataset_NMT = dataset_NMT.shuffle(buffer_size=10000)\n",
    "valid_dataset_NMT = valid_dataset_NMT.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for the Task - NMT\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "encoder_nmt = Encoder(english_vocab_table.size(), 256, 512, 'gru')\n",
    "decoder_nmt = Decoder_NMT(german_vocab_table.size(), 256, 512, 'gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back the encoder trained after the cp part\n",
    "checkpoint_dir = './encoder_cp'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_nmt, optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the new trained model at this checkpoint\n",
    "import os\n",
    "checkpoint_dir = './encoder_nmt'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_nmt, optimizer_step=tf.train.get_or_create_global_step())\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model for the NMT part\n",
    "NUM_EPOCHS = 1\n",
    "STATS_STEPS = 50\n",
    "EVAL_STEPS = 500\n",
    "\n",
    "valid_ppl = compute_ppl_nmt(encoder_nmt, decoder_nmt, valid_dataset_NMT)\n",
    "log_msg(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    dataset_NMT = dataset_NMT.shuffle(buffer_size = 10000)\n",
    "    for step_num, datum in enumerate(dataset_NMT, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun_nmt(encoder_nmt, decoder_nmt, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            log_msg(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl_nmt(encoder_nmt, decoder_nmt, valid_dataset_NMT)\n",
    "            \n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "                log_msg(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                log_msg(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "                \n",
    "        \n",
    "    log_msg(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the final trained model\n",
    "checkpoint_dir = './encoder_nmt'\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder_nmt, optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the final word embeddings of the encoder\n",
    "word_embeddings = encoder_nmt.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the embeddings of all the words in the vocab\n",
    "all_embeddings = word_embeddings([i for i in range(int(english_vocab_table.size()))]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an invert map - index2word - so that it can be printed back as a file\n",
    "keys = english_vocab_table.export()[0].numpy()\n",
    "values = english_vocab_table.export()[1].numpy()\n",
    "index2word = {v: k for k, v in zip(keys, values)}\n",
    "word2index = {k: v for k, v in zip(keys, values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the sklearn cosine similarity, return the distances and indices of the closest k points from any fixed point\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def cosine_knn(corpus_vector, queries_vector, k=6):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='cosine')\n",
    "    nbrs.fit(corpus_vector)\n",
    "    distances, indices = nbrs.kneighbors(queries_vector)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only the indices\n",
    "closest_indices = cosine_knn(all_embeddings, all_embeddings)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the file with 5 closest neighbours of each word in the vocab\n",
    "def create_closest_word_file(closest_indices, english_vocab_table, index2word):\n",
    "    with open('closest_neighbours.txt', 'w+') as f:\n",
    "        for i in range(int(english_vocab_table.size())):\n",
    "            curr_word = index2word[i]\n",
    "            closest_points = closest_indices[i][1:]\n",
    "            string = str(curr_word.decode(\"utf-8\")) + ', '\n",
    "            for j, nbr in enumerate(list(closest_points)):\n",
    "                neighbour = str(index2word[nbr].decode(\"utf-8\"))\n",
    "                if(j == len(list(closest_points)) - 1):\n",
    "                    string += neighbour\n",
    "                else:\n",
    "                    string += neighbour + ', '\n",
    "            string += '\\n'\n",
    "            f.write(string)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_closest_word_file(closest_indices, english_vocab_table, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
