{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import pickle\n",
    "tf.logging.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PATHs\n",
    "PATH_TO_SENTEVAL = '../'\n",
    "PATH_TO_DATA = './data_evaluation'\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SentEval\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, V, d):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "    \n",
    "    def call(self, word_indexes):\n",
    "        return tf.cast(tf.nn.embedding_lookup(self.W, word_indexes), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticRNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(StaticRNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, state, word_vectors, num_words):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs, final_state = tf.nn.static_rnn(cell=self.cell, initial_state = state, inputs=word_vectors_time, sequence_length=num_words, dtype=tf.float32)\n",
    "        return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embedding = Embedding(V, d)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        \n",
    "    def call(self, word_vector, word_length):\n",
    "        word_vectors = self.word_embedding(word_vector)\n",
    "        rnn_outputs_time, final_state = self.rnn(None, word_vectors, word_length)\n",
    "        output = []\n",
    "        for i in range(int(tf.size(word_length))):\n",
    "            output.append(rnn_outputs_time[int(word_length[i]) - 1][i])\n",
    "        t = tf.convert_to_tensor(output, dtype=tf.float32)\n",
    "        return t, final_state, self.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creae the dataset for each batch of sentences\n",
    "def create_dataset(sentences, vocab_table, batch_size):\n",
    "    sentences = tf.convert_to_tensor(sentences)\n",
    "    dataset = tf.data.TextLineDataset.from_tensor_slices(sentences)\n",
    "    dataset = dataset.map(lambda sentence: (\n",
    "        vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[0]]).values),\n",
    "        tf.size(vocab_table.lookup(tf.string_split([(tf.string_split([sentence],',')).values[0]]).values))\n",
    "                         ))\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([None], []))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the outputs of the encoder\n",
    "def batcher(params, batch):\n",
    "    batch = [' '.join(sent) if sent != [] else '.' for sent in batch]\n",
    "    dataset = create_dataset(batch, params['vocab'], params['classifier']['batch_size'])\n",
    "    datum = next(iter(dataset))\n",
    "    embeddings,_,_ = params['encoder'](datum[0], datum[1])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define senteval params\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                 'tenacity': 3, 'epoch_size': 2}\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #load the english vocab\n",
    "    from tensorflow.python.ops import lookup_ops\n",
    "    english_vocab_file = './data/english_vocab.txt'\n",
    "    english_vocab_table = lookup_ops.index_table_from_file(english_vocab_file, default_value=0)\n",
    "    params_senteval['vocab'] = english_vocab_table\n",
    "    \n",
    "    #loading the final trained model\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "    encoder_nmt = Encoder(english_vocab_table.size(), 256, 512, 'gru')\n",
    "    checkpoint_dir = './encoder_nmt'\n",
    "    root = tfe.Checkpoint(optimizer=opt, model=encoder_nmt, optimizer_step=tf.train.get_or_create_global_step())\n",
    "    root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    params_senteval['encoder'] = encoder_nmt\n",
    "    \n",
    "    \n",
    "    #running the evalutaton tasks\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'TREC']\n",
    "    results = se.eval(transfer_tasks)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
